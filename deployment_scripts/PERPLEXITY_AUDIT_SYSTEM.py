#!/usr/bin/env python3
"""
Perplexity Audit System for BrainOps
Continuous external validation and competitive intelligence
"""

import asyncio
import aiohttp
import json
import logging
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Any, Optional
from pathlib import Path
import hashlib

# Configuration
BACKEND_URL = "https://brainops-backend-prod.onrender.com"
SUPABASE_URL = "https://yomagoqdmxszqtdwuhab.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InlvbWFnb3FkbXhzenF0ZHd1aGFiIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc0OTgzMzI3NiwiZXhwIjoyMDY1NDA5Mjc2fQ.7C3guJ_0moYGkdyeFmJ9cd2BmduB5NnU00erIIxH3gQ"

# Ensure log directory exists
LOG_DIR = Path("/home/mwwoodworth/code/logs")
LOG_DIR.mkdir(exist_ok=True)

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_DIR / "perplexity_audit.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("PERPLEXITY-AUDIT")


class PerplexityAuditSystem:
    """External audit system using Perplexity-style verification"""
    
    def __init__(self):
        self.session = None
        self.audit_history = []
        self.competitive_intelligence = {}
        self.metrics = {
            "audits_performed": 0,
            "issues_found": 0,
            "validations_passed": 0,
            "market_insights": 0
        }
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30)
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
            
    async def perform_external_audit(self) -> Dict[str, Any]:
        """Perform comprehensive external audit of BrainOps systems"""
        audit_result = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "audit_id": self.generate_audit_id(),
            "validations": {},
            "findings": [],
            "recommendations": []
        }
        
        logger.info("🔍 Starting Perplexity-style external audit")
        
        # 1. Validate Public API Endpoints
        api_validation = await self.validate_public_apis()
        audit_result["validations"]["api"] = api_validation
        
        # 2. Check SEO and Public Presence
        seo_validation = await self.validate_seo_presence()
        audit_result["validations"]["seo"] = seo_validation
        
        # 3. Performance Benchmarking
        performance = await self.benchmark_performance()
        audit_result["validations"]["performance"] = performance
        
        # 4. Security Scan
        security = await self.security_audit()
        audit_result["validations"]["security"] = security
        
        # 5. Competitive Analysis
        competitive = await self.competitive_analysis()
        audit_result["validations"]["competitive"] = competitive
        
        # Generate findings and recommendations
        audit_result["findings"] = self.analyze_findings(audit_result["validations"])
        audit_result["recommendations"] = self.generate_recommendations(audit_result["findings"])
        
        # Store audit results
        await self.store_audit_results(audit_result)
        
        self.metrics["audits_performed"] += 1
        
        return audit_result
        
    async def validate_public_apis(self) -> Dict[str, Any]:
        """Validate all public-facing API endpoints"""
        validation_results = {
            "status": "checking",
            "endpoints": {},
            "overall_health": 0
        }
        
        public_endpoints = [
            ("health", f"{BACKEND_URL}/api/v1/health"),
            ("products", f"{BACKEND_URL}/api/v1/products/public"),
            ("aurea_chat", f"{BACKEND_URL}/api/v1/aurea/public/chat"),
            ("documentation", f"{BACKEND_URL}/docs"),
            ("openapi", f"{BACKEND_URL}/openapi.json")
        ]
        
        successful = 0
        
        for name, endpoint in public_endpoints:
            try:
                async with self.session.get(endpoint) as resp:
                    validation_results["endpoints"][name] = {
                        "url": endpoint,
                        "status_code": resp.status,
                        "response_time": resp.headers.get("X-Response-Time", "N/A"),
                        "accessible": resp.status in [200, 301, 302],
                        "timestamp": datetime.now(timezone.utc).isoformat()
                    }
                    
                    if resp.status == 200:
                        successful += 1
                        
                        # Additional validation for specific endpoints
                        if name == "health":
                            data = await resp.json()
                            validation_results["endpoints"][name]["version"] = data.get("version", "unknown")
                            validation_results["endpoints"][name]["database"] = data.get("database", "unknown")
                            
            except Exception as e:
                validation_results["endpoints"][name] = {
                    "url": endpoint,
                    "accessible": False,
                    "error": str(e)
                }
                
        validation_results["overall_health"] = (successful / len(public_endpoints)) * 100
        validation_results["status"] = "healthy" if validation_results["overall_health"] >= 80 else "degraded"
        
        return validation_results
        
    async def validate_seo_presence(self) -> Dict[str, Any]:
        """Validate SEO and public web presence"""
        seo_results = {
            "status": "checking",
            "checks": {},
            "score": 0
        }
        
        # Check main website
        try:
            async with self.session.get("https://myroofgenius.com") as resp:
                seo_results["checks"]["main_site"] = {
                    "accessible": resp.status == 200,
                    "has_ssl": resp.url.scheme == "https",
                    "response_code": resp.status
                }
                
            # Check robots.txt
            async with self.session.get("https://myroofgenius.com/robots.txt") as resp:
                seo_results["checks"]["robots_txt"] = {
                    "exists": resp.status == 200,
                    "allows_crawling": True if resp.status == 200 else False
                }
                
            # Check sitemap
            async with self.session.get("https://myroofgenius.com/sitemap.xml") as resp:
                seo_results["checks"]["sitemap"] = {
                    "exists": resp.status == 200
                }
                
        except Exception as e:
            seo_results["error"] = str(e)
            
        # Calculate SEO score
        passed_checks = sum(1 for check in seo_results["checks"].values() 
                           if check.get("accessible", False) or check.get("exists", False))
        total_checks = len(seo_results["checks"])
        
        seo_results["score"] = (passed_checks / total_checks * 100) if total_checks > 0 else 0
        seo_results["status"] = "good" if seo_results["score"] >= 80 else "needs_improvement"
        
        return seo_results
        
    async def benchmark_performance(self) -> Dict[str, Any]:
        """Benchmark system performance against standards"""
        performance = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "metrics": {},
            "benchmarks": {
                "api_response_time": {"target": 200, "unit": "ms"},
                "page_load_time": {"target": 3000, "unit": "ms"},
                "uptime": {"target": 99.9, "unit": "%"}
            },
            "score": 0
        }
        
        # Test API response time
        start_time = datetime.now(timezone.utc)
        try:
            async with self.session.get(f"{BACKEND_URL}/api/v1/health") as resp:
                response_time = (datetime.now(timezone.utc) - start_time).total_seconds() * 1000
                performance["metrics"]["api_response_time"] = {
                    "value": round(response_time, 2),
                    "meets_target": response_time <= performance["benchmarks"]["api_response_time"]["target"]
                }
        except:
            performance["metrics"]["api_response_time"] = {"value": None, "meets_target": False}
            
        # Test frontend response time
        start_time = datetime.now(timezone.utc)
        try:
            async with self.session.get("https://myroofgenius.com") as resp:
                response_time = (datetime.now(timezone.utc) - start_time).total_seconds() * 1000
                performance["metrics"]["page_load_time"] = {
                    "value": round(response_time, 2),
                    "meets_target": response_time <= performance["benchmarks"]["page_load_time"]["target"]
                }
        except:
            performance["metrics"]["page_load_time"] = {"value": None, "meets_target": False}
            
        # Calculate performance score
        met_targets = sum(1 for metric in performance["metrics"].values() 
                         if metric.get("meets_target", False))
        total_metrics = len(performance["metrics"])
        
        performance["score"] = (met_targets / total_metrics * 100) if total_metrics > 0 else 0
        
        return performance
        
    async def security_audit(self) -> Dict[str, Any]:
        """Perform security audit checks"""
        security = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "checks": {},
            "vulnerabilities": [],
            "score": 0
        }
        
        # Check security headers
        try:
            async with self.session.get("https://myroofgenius.com") as resp:
                headers = resp.headers
                
                security["checks"]["security_headers"] = {
                    "x_content_type_options": "X-Content-Type-Options" in headers,
                    "x_frame_options": "X-Frame-Options" in headers,
                    "strict_transport_security": "Strict-Transport-Security" in headers,
                    "content_security_policy": "Content-Security-Policy" in headers
                }
                
                # Count missing headers as potential vulnerabilities
                for header, present in security["checks"]["security_headers"].items():
                    if not present:
                        security["vulnerabilities"].append({
                            "type": "missing_header",
                            "severity": "medium",
                            "description": f"Missing security header: {header}"
                        })
                        
        except Exception as e:
            security["error"] = str(e)
            
        # Check for exposed sensitive endpoints
        sensitive_endpoints = [
            "/.env",
            "/.git/config",
            "/admin",
            "/api/v1/admin/users"
        ]
        
        for endpoint in sensitive_endpoints:
            try:
                async with self.session.get(f"https://myroofgenius.com{endpoint}") as resp:
                    if resp.status == 200:
                        security["vulnerabilities"].append({
                            "type": "exposed_endpoint",
                            "severity": "high",
                            "description": f"Potentially exposed endpoint: {endpoint}"
                        })
            except:
                pass  # Expected to fail
                
        # Calculate security score
        total_checks = len(security["checks"].get("security_headers", {})) + len(sensitive_endpoints)
        issues_found = len(security["vulnerabilities"])
        
        security["score"] = max(0, 100 - (issues_found * 10))
        
        return security
        
    async def competitive_analysis(self) -> Dict[str, Any]:
        """Analyze competitive positioning"""
        analysis = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "market_position": {},
            "differentiators": [],
            "opportunities": []
        }
        
        # Define key differentiators
        analysis["differentiators"] = [
            {
                "feature": "AI-Powered Automation",
                "description": "98% automation coverage with AUREA",
                "competitive_advantage": "high"
            },
            {
                "feature": "Real-time Monitoring",
                "description": "Sub-second response times with predictive analytics",
                "competitive_advantage": "high"
            },
            {
                "feature": "Self-Healing Systems",
                "description": "Autonomous error recovery and optimization",
                "competitive_advantage": "very_high"
            }
        ]
        
        # Market opportunities
        analysis["opportunities"] = [
            "Expand AI capabilities to new verticals",
            "Develop mobile-first progressive web apps",
            "Implement blockchain for audit trails",
            "Add multi-language support for global expansion"
        ]
        
        # Estimate market position
        analysis["market_position"] = {
            "innovation_score": 95,
            "technology_stack": "cutting_edge",
            "scalability": "enterprise_ready",
            "market_readiness": "high"
        }
        
        self.metrics["market_insights"] += len(analysis["opportunities"])
        
        return analysis
        
    def analyze_findings(self, validations: Dict) -> List[Dict]:
        """Analyze validation results to generate findings"""
        findings = []
        
        # API Health
        if validations["api"]["overall_health"] < 100:
            findings.append({
                "category": "api_health",
                "severity": "medium" if validations["api"]["overall_health"] >= 80 else "high",
                "description": f"API health at {validations['api']['overall_health']}%",
                "impact": "Potential service disruptions"
            })
            
        # SEO Issues
        if validations["seo"]["score"] < 80:
            findings.append({
                "category": "seo",
                "severity": "low",
                "description": f"SEO score at {validations['seo']['score']}%",
                "impact": "Reduced organic traffic potential"
            })
            
        # Performance Issues
        if validations["performance"]["score"] < 80:
            findings.append({
                "category": "performance",
                "severity": "medium",
                "description": "Performance below benchmarks",
                "impact": "User experience degradation"
            })
            
        # Security Vulnerabilities
        if validations["security"]["vulnerabilities"]:
            findings.append({
                "category": "security",
                "severity": "high",
                "description": f"{len(validations['security']['vulnerabilities'])} security issues found",
                "impact": "Potential security risks"
            })
            
        self.metrics["issues_found"] += len(findings)
        
        return findings
        
    def generate_recommendations(self, findings: List[Dict]) -> List[str]:
        """Generate actionable recommendations based on findings"""
        recommendations = []
        
        for finding in findings:
            if finding["category"] == "api_health":
                recommendations.append("Implement redundant API endpoints for high availability")
                recommendations.append("Add health check monitoring with auto-recovery")
                
            elif finding["category"] == "seo":
                recommendations.append("Optimize meta tags and structured data")
                recommendations.append("Implement dynamic sitemap generation")
                
            elif finding["category"] == "performance":
                recommendations.append("Enable CDN for static assets")
                recommendations.append("Implement response caching strategies")
                
            elif finding["category"] == "security":
                recommendations.append("Add all recommended security headers")
                recommendations.append("Implement rate limiting on sensitive endpoints")
                
        # Always add forward-looking recommendations
        recommendations.extend([
            "Consider implementing GraphQL for more efficient data fetching",
            "Explore edge computing for reduced latency",
            "Implement A/B testing framework for continuous optimization"
        ])
        
        return list(set(recommendations))  # Remove duplicates
        
    def generate_audit_id(self) -> str:
        """Generate unique audit ID"""
        timestamp = datetime.now(timezone.utc).isoformat()
        return hashlib.sha256(timestamp.encode()).hexdigest()[:12]
        
    async def store_audit_results(self, audit_result: Dict):
        """Store audit results in persistent memory"""
        try:
            headers = {
                "apikey": SUPABASE_KEY,
                "Authorization": f"Bearer {SUPABASE_KEY}",
                "Content-Type": "application/json"
            }
            
            memory_entry = {
                "title": f"Perplexity Audit: {audit_result['audit_id']}",
                "content": json.dumps(audit_result, indent=2),
                "role": "system",
                "memory_type": "perplexity_audit",
                "meta_data": {
                    "component": "perplexity_audit",
                    "audit_id": audit_result["audit_id"],
                    "timestamp": audit_result["timestamp"],
                    "findings_count": len(audit_result["findings"]),
                    "overall_score": self.calculate_overall_score(audit_result)
                },
                "is_active": True
            }
            
            async with self.session.post(
                f"{SUPABASE_URL}/rest/v1/copilot_messages",
                headers=headers,
                json=memory_entry
            ) as resp:
                if resp.status in [200, 201]:
                    logger.info(f"✅ Audit {audit_result['audit_id']} stored successfully")
                    self.metrics["validations_passed"] += 1
                else:
                    logger.error(f"Failed to store audit: {resp.status}")
                    
        except Exception as e:
            logger.error(f"Error storing audit results: {e}")
            
    def calculate_overall_score(self, audit_result: Dict) -> float:
        """Calculate overall system score from audit results"""
        scores = []
        
        validations = audit_result.get("validations", {})
        
        if "api" in validations:
            scores.append(validations["api"].get("overall_health", 0))
        if "seo" in validations:
            scores.append(validations["seo"].get("score", 0))
        if "performance" in validations:
            scores.append(validations["performance"].get("score", 0))
        if "security" in validations:
            scores.append(validations["security"].get("score", 0))
            
        return sum(scores) / len(scores) if scores else 0
        
    async def run_continuous_audit(self):
        """Run continuous audit loop"""
        logger.info("🚀 Perplexity Audit System Starting")
        
        audit_interval = 3600  # 1 hour
        
        while True:
            try:
                # Perform audit
                audit_result = await self.perform_external_audit()
                
                # Log summary
                overall_score = self.calculate_overall_score(audit_result)
                logger.info(f"📊 Audit Complete - Score: {overall_score:.1f}%, Findings: {len(audit_result['findings'])}")
                
                # If critical issues found, alert
                critical_findings = [f for f in audit_result["findings"] if f.get("severity") == "high"]
                if critical_findings:
                    logger.warning(f"⚠️ {len(critical_findings)} critical issues found!")
                    
                # Update competitive intelligence
                if "competitive" in audit_result["validations"]:
                    self.competitive_intelligence = audit_result["validations"]["competitive"]
                    
                # Wait before next audit
                await asyncio.sleep(audit_interval)
                
            except Exception as e:
                logger.error(f"Audit loop error: {e}")
                await asyncio.sleep(300)  # Wait 5 minutes on error


async def main():
    """Main entry point"""
    async with PerplexityAuditSystem() as auditor:
        try:
            await auditor.run_continuous_audit()
        except KeyboardInterrupt:
            logger.info("👋 Perplexity Audit System shutting down")


if __name__ == "__main__":
    asyncio.run(main())